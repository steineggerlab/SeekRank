{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsbNNWuiBAuq"
      },
      "source": [
        "# Installing Facebook embeddings template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwwpHDgUixe0"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/facebookresearch/esm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L7KgzmFAjEPE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import esm\n",
        "\n",
        "# Load 34 layer model\n",
        "model, alphabet = esm.pretrained.esm1_t34_670M_UR50S()\n",
        "# If you have a GPU, put the model on it\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WLZEFi4ELNRt"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M90c7UnGKUz2"
      },
      "outputs": [],
      "source": [
        "def batch(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx:min(ndx + n, l)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDuv4cuTBU0i"
      },
      "source": [
        "# PART 1: Generating prediction model\n",
        "\n",
        "# 1.1 Preparing data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV4u_FtppuRz"
      },
      "outputs": [],
      "source": [
        "# Prepare data (two protein sequences)\n",
        "\n",
        "FASTA_PATH=\"./training_seqs.fasta\" # Fasta to train\n",
        "\n",
        "data=[]\n",
        "ys = []\n",
        "Xs = []\n",
        "for header, sequence in esm.data.read_fasta(FASTA_PATH):\n",
        "  data.append((header, sequence))\n",
        "  body = (header.split(' '))[-1]\n",
        "  ys.append(float(body))\n",
        "print(ys)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWV2t8O9BkTt"
      },
      "source": [
        "# 1.2 Building embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4q99U7HLdPJB"
      },
      "outputs": [],
      "source": [
        "sequence_embeddings = []\n",
        "# build embeddings\n",
        "for batch_seqs in batch(data,10):\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_seqs)\n",
        "\n",
        "    # Extract per-residue embeddings (on GPU)\n",
        "    # batch_tokens_cuda = batch_tokens.to(device=\"cuda\", non_blocking=True)\n",
        "    batch_tokens_cuda = batch_tokens.to(device, non_blocking=True)\n",
        "    with torch.no_grad():\n",
        "        results = model(batch_tokens_cuda, repr_layers=[34])\n",
        "    token_embeddings = results[\"representations\"][34]\n",
        "    # Generate per-sequence embeddings via averaging\n",
        "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "    for i, (_, seq) in enumerate(batch_seqs):\n",
        "        sequence_embeddings.append(token_embeddings[i, 1:len(seq) + 1].mean(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kw1Oou7Yo1Dt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1280\n"
          ]
        }
      ],
      "source": [
        "print(len(sequence_embeddings[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtkb9Oi1Bvcr"
      },
      "source": [
        "# 1.3 Creating Training set & Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J5-Fo9C4KZOL"
      },
      "outputs": [],
      "source": [
        "# split training and test set\n",
        "Xs=[t.cpu().data.numpy() for t in sequence_embeddings]\n",
        "train_size = 0.8\n",
        "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, train_size=train_size, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEHcrGIpB6UF"
      },
      "source": [
        "# 1.4 Beginning of the training block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Pesb6f6xXRo_"
      },
      "outputs": [],
      "source": [
        "knn_grid = {\n",
        "    'n_neighbors': [5, 10],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
        "    'leaf_size' : [15, 30],\n",
        "    'p' : [1, 2],\n",
        "}\n",
        "\n",
        "svm_grid = {\n",
        "    'C' : [0.1, 1.0, 10.0],\n",
        "    'kernel' :['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'degree' : [3],\n",
        "    'gamma': ['scale'],\n",
        "}\n",
        "\n",
        "rfr_grid = {\n",
        "    'n_estimators' : [100],\n",
        "    'criterion' : ['squared_error', 'absolute_error'],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'min_samples_split' : [2, 10],\n",
        "    'min_samples_leaf': [1, 4]\n",
        "}\n",
        "lgr_grid = {\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NIDqa1yXY7M"
      },
      "outputs": [],
      "source": [
        "# Training Block!!!\n",
        "cls_list = [KNeighborsRegressor, SVR, RandomForestRegressor]\n",
        "param_grid_list = [knn_grid, svm_grid, rfr_grid]\n",
        "result_list = []\n",
        "grid_list = []\n",
        "for cls_name, param_grid in zip(cls_list, param_grid_list):\n",
        "    print(cls_name)\n",
        "    grid = GridSearchCV(\n",
        "        estimator = cls_name(),\n",
        "        param_grid = param_grid,\n",
        "        scoring = 'r2',\n",
        "        verbose = 1,\n",
        "        n_jobs = -1 # use all available cores\n",
        "    )\n",
        "    grid.fit(Xs_train, ys_train)\n",
        "    result_list.append(pd.DataFrame.from_dict(grid.cv_results_))\n",
        "    grid_list.append(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsaUveq3CDWp"
      },
      "source": [
        "# 1.5 Testing the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lS0CXQoCnJi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "\n",
        "# Assuming grid_list, Xs_test, and ys_test are already defined\n",
        "\n",
        "for i, grid in enumerate(grid_list):\n",
        "    print(grid.best_estimator_)\n",
        "    print()\n",
        "\n",
        "    # Predictions\n",
        "    preds = grid.predict(Xs_test)\n",
        "\n",
        "    # Calculate Spearman's correlation\n",
        "    rho, p_value = scipy.stats.spearmanr(ys_test, preds)\n",
        "\n",
        "    # Create a DataFrame and save to CSV\n",
        "    df = pd.DataFrame({'Actual Kcat/Km': ys_test, 'Predicted Kcat/Km': preds})\n",
        "    csv_filename = f'grid_element_{i}_kcat_km_data.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f'Data saved to {csv_filename}')\n",
        "\n",
        "    # Create scatter plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(ys_test, preds, alpha=0.7)\n",
        "    plt.xlabel('Actual Kcat/Km')\n",
        "    plt.ylabel('Predicted Kcat/Km')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Annotate with Spearman's rho\n",
        "    plt.annotate(f'Spearman\\'s rho = {rho:.2f}\\nP-value = {p_value:.2e}',\n",
        "                 xy=(0.05, 0.85), xycoords='axes fraction',\n",
        "                 fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='black', facecolor='white'))\n",
        "\n",
        "    # Save plot as an image\n",
        "    img_filename = f'grid_element_{i}_kcat_km_plot.png'\n",
        "    plt.savefig(img_filename)\n",
        "    print(f'Plot saved to {img_filename}')\n",
        "    plt.close()\n",
        "\n",
        "    print('\\n', '-' * 80, '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZwO1-seXzpj"
      },
      "outputs": [],
      "source": [
        "for i, grid in enumerate(grid_list):\n",
        "    print(grid.best_estimator_)\n",
        "    print()\n",
        "    preds = grid.predict(Xs_test)\n",
        "    print(f'{scipy.stats.spearmanr(ys_test, preds)}')\n",
        "    print('\\n', '-' * 80, '\\n')\n",
        "    # Calculate Spearman's correlation\n",
        "    rho, p_value = scipy.stats.spearmanr(ys_test, preds)\n",
        "\n",
        "    # Create a DataFrame and save to CSV\n",
        "    df = pd.DataFrame({'Actual Values': ys_test, 'Predicted Values': preds})\n",
        "    csv_filename = f'grid_element_{i}_data.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f'Data saved to {csv_filename}')\n",
        "\n",
        "    # Create scatter plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(ys_test, preds, alpha=0.7)\n",
        "    plt.title('Spearman Correlation between Actual and Predicted Values')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Annotate with Spearman's rho\n",
        "    plt.annotate(f'Spearman\\'s rho = {rho:.2f}\\nP-value = {p_value:.2e}',\n",
        "              xy=(0.05, 0.75), xycoords='axes fraction',\n",
        "              fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='black', facecolor='white'))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQFjjQB6MPIq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "\n",
        "\n",
        "# Calculate Spearman's correlation\n",
        "rho, p_value = scipy.stats.spearmanr(ys_test, preds)\n",
        "\n",
        "# Create scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(ys_test, preds, alpha=0.7)\n",
        "plt.title('Spearman Correlation between Actual and Predicted Values')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.grid(True)\n",
        "\n",
        "# Annotate with Spearman's rho\n",
        "plt.annotate(f'Spearman\\'s rho = {rho:.2f}\\nP-value = {p_value:.2e}',\n",
        "              xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "              fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='black', facecolor='white'))\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq6iTaHXf-BQ"
      },
      "outputs": [],
      "source": [
        "topredict=[('wt','MEPSSLELPADTVQRIAAELKCHPTDERVALHLDEEDKLRHFRECFYIPKIQDLPPVDLSLVNKDENAIYFLGNSLGLQPKMVKTYLEEELDKWAKIAAYGHEVGKRPWITGDESIVGLMKDIVGANEKEIALMNALTVNLHLLMLSFFKPTPKRYKILLEAKAFPSDHYAIESQLQLHGLNIEESMRMIKPREGEETLRIEDILEVIEKEGDSIAVILFSGVHFYTGQHFNIPAITKAGQAKGCYVGFDLAHAVGNVELYLHDWGVDFACWCSYKYLNAGAGGIAGAFIHEKHAHTIKPALVGWFGHELSTRFKMDNKLQLIPGVCGFRISNPPILLVCSLHASLEIFKQATMKALRKKSVLLTGYLEYLIKHNYGKDKAATKKPVVNIITPSHVEERGCQLTITFSVPNKDVFQELEKRGVVCDKRNPNGIRVAPVPLYNSFHDVYKFTNLLTSILDSAETKN'),('best_patent','MEPSSLELPADTVQRIAAELKCHPTDERVALHLDEEDKLRHFRECFYIPKIQDLPPVDLSLVNKDEDAIYFNGNSLGLQPKMVKTYLEEELDKWAKIAINGWFEGDSPWIHYDESIVGLMKDIVGANEKEIVLMNTLTVNLHLLMLSFFKPTPKRYKILLEAKAFPSDHYAIESQLQLHGLNIEESMRIIKPREGEETLRIEDILEVIEKEGDSIAVILFSGIHYYTGQHFNIPAITKAGQAKGCYVGFDLAHAVGNVELYLHDWGVDFACWCGYKYLNSSPGGIAGAFIHEKHAHTIKPALVGWFGHELSTRFKMDNKLQLIPGVCGFRCSTPPILLVCILHASLEIFKQATMKALRKKSVLLTGYLEYLIKHNYGKDKAATKKPVVNIITPSHVEERGCQLTLTFNVPNKDVFQELEKRGVVCDKRNPNGIRVAPVPLYNSFHDVYKFTNLLTSILDSAETKN'),('best_mut','MEPSSLELPADTVQRIAAELKCHPTDERVALHLDEEDKLRHFRECFYIPKIQDLPPVDLSLVNKDEDAIYFNGNSLGLQPKMVKTYREEELDKWAKIAINGWFEGDSPWIHYDESIVGLMKDIVGANEKEIVLWYTLTHMLHLLMLSFFKPTPKRYKILLYAKAFPSDHYAIESQLQLHGLNIEESMRIIKPREGEETLRIEDILEVIEKEGDSIAVITFSGIHYMTGQHFNIPAITKALQAKGCYVGFDQAHAVGNVELYLHDWGVDFACNCGYKYLNSSPGWIQGWFCHEKHAHTIKPALVGWFGHELSTRFKMDNKLQLIPGVCGFRCSTPNHWLVCILHAPLENFKQATMKALRKKSVLLTGYLEYLIKHNYGKDKAATKKPVVNIITPSHVEERGCQLTLTFNVPNKDVFQELEKRGVVCDKRNPNGIRVAPVPLYNSFHDVYKFTNLLTSILDSAETKN'),('worst_mut','MEPSSLELPADTVQRIAAELKCHPTDERVALHLDEEDKLRHFRECFYIPKIQDLPPVDLSLVNKDEDAIYFNGNSLGLQPKMVKTYYEEELDKWAKIAINGWFEGDSPWIHYDESIVGLMKDIVGANEKEIVLYFTLTDQLHLLMLSFFKPTPKRYKILLNAKAFPSDHYAIESQLQLHGLNIEESMRIIKPREGEETLRIEDILEVIEKEGDSIAVIMFSGIHYETGQHFNIPAITKAMQAKGCYVGFDPAHAVGNVELYLHDWGVDFACVCGYKYLNSSPGIINGRFDHEKHAHTIKPALVGWFGHELSTRFKMDNKLQLIPGVCGFRCSTPKRKLVCILHAHLELFKQATMKALRKKSVLLTGYLEYLIKHNYGKDKAATKKPVVNIITPSHVEERGCQLTLTFNVPNKDVFQELEKRGVVCDKRNPNGIRVAPVPLYNSFHDVYKFTNLLTSILDSAETKN'),('var_93','MEPSPLELPADTVQRIASELRCHPTDERVALRLDEEDELRHFREYFYIPKMQDLPPIDLSLVNKDENAIYFLGNSLGLQPKMVKTYLEEELDKWAKMGAYGHEVGKRPWITGDETIVGLMTDIVGANEKEIALMNGLTVNLHLLLLSFFKPTPKRYKILLEAKAFPSDHYAIESQLQLHGLNVEKSMRIIKPREGEETLRTEDILEVIEKEGDSIAVILFSGVHFYTGQLFNIPAITKAGQAKGCFVGFDLAHAVGNVELHLHDWGVDFACWCSYKYLNSGAGGLAGAFVHEKHAYTIKPALVGWFGHELSTRFKMDNKLQLIPGVNGFRISNPPILLVCSLHASLEIFKQATMKALRRKSILLTGYLEYLIKHYYSKDKAETKKPIVNIITPSRIEERGCQLTLTFSVPMKYVFQELEKRGVVCDKREPNGIRVAPVPLYNSFHDVYKFIELLTSVLDSAETK')]\n",
        "\n",
        "for batch_seqs in batch(topredict, 1):\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(batch_seqs)\n",
        "  # build embeddings\n",
        "  # batch_tokens_cuda = batch_tokens.to(device=\"cuda\", non_blocking=True)\n",
        "  batch_tokens_cuda = batch_tokens.to(device, non_blocking=True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    results = model(batch_tokens_cuda, repr_layers=[34])\n",
        "  token_embeddings = results[\"representations\"][34]\n",
        "\n",
        "  # Generate per-sequence embeddings via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_embeddings = []\n",
        "  for i, (_, seq) in enumerate(batch_seqs):\n",
        "    sequence_embeddings.append(token_embeddings[i, 1:len(seq) + 1].mean(0))\n",
        "\n",
        "  predict_seqs_embeddings=[t.cpu().data.numpy() for t in sequence_embeddings]\n",
        "  preds=[]\n",
        "  for grid in grid_list:\n",
        "    pred = grid.predict(predict_seqs_embeddings)\n",
        "    preds.append(pred)\n",
        "  for i in range(0, len(batch_seqs)):\n",
        "    #f.write(\"{} {} {} {}\\n\".format(batch_seqs[i][0], preds[0][i],  preds[1][i],  preds[2][i]))\n",
        "    print(batch_seqs[i][0], preds[0][i], preds[1][i], preds[2][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAEuEnRMDE8V"
      },
      "source": [
        "# PART 2: Building embeddings (completely new data) & feed it into the prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Install colabfold and search training sequences against colabfold database with mmseqs2\n",
        "!pip install \"colabfold[alphafold] @ git+https://github.com/sokrypton/ColabFold\"\n",
        "import requests\n",
        "import hashlib\n",
        "import tarfile\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "\n",
        "import random\n",
        "import tqdm.notebook\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import matplotlib.patheffects\n",
        "from matplotlib import collections as mcoll\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import tqdm.notebook\n",
        "import random\n",
        "import tarfile\n",
        "\n",
        "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
        "\n",
        "def run_mmseqs2(x, prefix, use_env=True, use_filter=True,\n",
        "                use_templates=False, filter=None, host_url=\"https://api.colabfold.com\"):\n",
        "  \n",
        "  def submit(seqs, mode, N=101):    \n",
        "    n,query = N,\"\"\n",
        "    for seq in seqs: \n",
        "      query += f\">{n}\\n{seq}\\n\"\n",
        "      n += 1\n",
        "      \n",
        "    while True:\n",
        "      try:\n",
        "        # https://requests.readthedocs.io/en/latest/user/advanced/#advanced\n",
        "        # \"good practice to set connect timeouts to slightly larger than a multiple of 3\"\n",
        "        res = requests.post(f'{host_url}/ticket/msa', data={'q':query,'mode': mode}, timeout=6.02)\n",
        "      except requests.exceptions.Timeout:\n",
        "        continue\n",
        "      break\n",
        "\n",
        "    try: out = res.json()\n",
        "    except ValueError: out = {\"status\":\"UNKNOWN\"}\n",
        "    return out\n",
        "\n",
        "  def status(ID):\n",
        "    while True:\n",
        "      try:\n",
        "        res = requests.get(f'{host_url}/ticket/{ID}', timeout=6.02)\n",
        "      except requests.exceptions.Timeout:\n",
        "        continue\n",
        "      break\n",
        "\n",
        "    try: out = res.json()\n",
        "    except ValueError: out = {\"status\":\"UNKNOWN\"}\n",
        "    return out\n",
        "\n",
        "  def download(ID, path):\n",
        "    while True:\n",
        "      try:\n",
        "        res = requests.get(f'{host_url}/result/download/{ID}', timeout=6.02)\n",
        "      except requests.exceptions.Timeout:\n",
        "        continue\n",
        "      break\n",
        "\n",
        "    with open(path,\"wb\") as out: out.write(res.content)\n",
        "  \n",
        "  # process input x\n",
        "  seqs = [x] if isinstance(x, str) else x\n",
        "  \n",
        "  # compatibility to old option\n",
        "  if filter is not None:\n",
        "    use_filter = filter\n",
        "    \n",
        "  # setup mode\n",
        "  if use_filter:\n",
        "    mode = \"env\" if use_env else \"all\"\n",
        "  else:\n",
        "    mode = \"env-nofilter\" if use_env else \"nofilter\"\n",
        "\n",
        "  mode += \"-m8output\"\n",
        "  \n",
        "  # define path\n",
        "  path = f\"{prefix}_{mode}\"\n",
        "  if not os.path.isdir(path): os.mkdir(path)\n",
        "\n",
        "  # call mmseqs2 api\n",
        "  tar_gz_file = f'{path}/out.tar.gz'\n",
        "  N,REDO = 101,True\n",
        "  \n",
        "  # deduplicate and keep track of order\n",
        "  seqs_unique = sorted(list(set(seqs)))\n",
        "  Ms = [N+seqs_unique.index(seq) for seq in seqs]\n",
        "  \n",
        "  # lets do it!\n",
        "  if not os.path.isfile(tar_gz_file):\n",
        "    TIME_ESTIMATE = 150 * len(seqs_unique)\n",
        "    with tqdm.notebook.tqdm(total=TIME_ESTIMATE, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "      while REDO:\n",
        "        pbar.set_description(\"SUBMIT\")\n",
        "        \n",
        "        # Resubmit job until it goes through\n",
        "        out = submit(seqs_unique, mode, N)\n",
        "        while out[\"status\"] in [\"UNKNOWN\",\"RATELIMIT\"]:\n",
        "          # resubmit\n",
        "          time.sleep(5 + random.randint(0,5))\n",
        "          out = submit(seqs_unique, mode, N)\n",
        "\n",
        "        if out[\"status\"] == \"ERROR\":\n",
        "          raise Exception(f'MMseqs2 API is giving errors. Please confirm your input is a valid protein sequence. If error persists, please try again an hour later.')\n",
        "\n",
        "        if out[\"status\"] == \"MAINTENANCE\":\n",
        "          raise Exception(f'MMseqs2 API is undergoing maintenance. Please try again in a few minutes.')\n",
        "\n",
        "        # wait for job to finish\n",
        "        ID,TIME = out[\"id\"],0\n",
        "        pbar.set_description(out[\"status\"])\n",
        "        while out[\"status\"] in [\"UNKNOWN\",\"RUNNING\",\"PENDING\"]:\n",
        "          t = 5 + random.randint(0,5)\n",
        "          time.sleep(t)\n",
        "          out = status(ID)    \n",
        "          pbar.set_description(out[\"status\"])\n",
        "          if out[\"status\"] == \"RUNNING\":\n",
        "            TIME += t\n",
        "            pbar.update(n=t)\n",
        "          #if TIME > 900 and out[\"status\"] != \"COMPLETE\":\n",
        "          #  # something failed on the server side, need to resubmit\n",
        "          #  N += 1\n",
        "          #  break\n",
        "        \n",
        "        if out[\"status\"] == \"COMPLETE\":\n",
        "          if TIME < TIME_ESTIMATE:\n",
        "            pbar.update(n=(TIME_ESTIMATE-TIME))\n",
        "          REDO = False\n",
        "\n",
        "      # Download results\n",
        "      download(ID, tar_gz_file)\n",
        "\n",
        "  # prep list of a3m files\n",
        "  m8_files = [f\"{path}/uniref.m8\"]\n",
        "  if use_env: m8_files.append(f\"{path}/bfd.mgnify30.metaeuk30.smag30.m8\")\n",
        "  \n",
        "  # extract a3m files\n",
        "  if not os.path.isfile(m8_files[0]):\n",
        "    with tarfile.open(tar_gz_file) as tar_gz:\n",
        "      tar_gz.extractall(path)  \n",
        "\n",
        "  m8_lines = {}\n",
        "  for m8_file in m8_files:\n",
        "    for line in open(m8_file,\"r\"):\n",
        "      if len(line) > 0:\n",
        "        if \"\\x00\" in line:\n",
        "          line = line.replace(\"\\x00\",\"\")\n",
        "        M = int(line.split()[0])\n",
        "        if M not in m8_lines: m8_lines[M] = []\n",
        "        m8_lines[M].append(line)\n",
        "  \n",
        "  # return results\n",
        "  m8_lines = [\"\".join(m8_lines[n]) for n in Ms]\n",
        "  \n",
        "  if isinstance(x, str):\n",
        "    return m8_lines[0]\n",
        "  else:\n",
        "    return m8_lines\n",
        "\n",
        "# Run mmseqs with the query sequence\n",
        "m8_output = run_mmseqs2([x[1] for x in data], \"tmp\")\n",
        "\n",
        "amino_acid = [\"G\", \"A\", \"L\", \"M\", \"K\", \"F\", \"W\", \"Q\", \"E\", \"S\", \"P\", \"V\", \"I\", \"C\", \"Y\", \"H\", \"R\", \"N\", \"D\", \"T\"]\n",
        "with open(\"./esm1b_target.m8\", 'w') as new:\n",
        "  for each in m8_output:\n",
        "    for line in each.split(\"\\n\"):\n",
        "      if line != \"\":\n",
        "        seq = line.split()[-1].upper()\n",
        "        new_seq = \"\"\n",
        "        for char in seq:\n",
        "          if char in amino_acid:\n",
        "            new_seq += char\n",
        "        new.write(\"%s %s\\n\"%(line.split()[1], new_seq))\n",
        "!sort -u -k2,2 esm1b_target.m8 | sort -k1,1 | awk 'BEGIN {cnt=2;name=\"\";} {if (name == $1) {print $1\"_colab_\"cnt, $2;cnt += 1} else {print;name=$1;cnt=2}}' > esm1b_nonredundant.m8\n",
        "\n",
        "# Gather the search result and make input file for esm1b\n",
        "#searched_sequences = []\n",
        "#searched_names = []\n",
        "with open(\"./esm1b_nonredundant.m8\") as f:\n",
        "  with open(\"./esm1b_target_input.fasta\", 'w') as new:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "      if len(line[:-1].split()[1]) < 1023:\n",
        "        new.write(\">%s\\n%s\\n\"%(line.split()[0], line[:-1].split()[1].upper()))\n",
        "      line = f.readline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4AvRQVkIgQPm"
      },
      "outputs": [],
      "source": [
        "METAGENOME_FASTA_PATH=\"./esm1b_target_input.fasta\"\n",
        "\n",
        "\n",
        "topredict = []\n",
        "with open('./target_prediction.tsv', 'w') as f:\n",
        "  for header, sequence in esm.data.read_fasta(METAGENOME_FASTA_PATH):\n",
        "    topredict.append((header, sequence))\n",
        "\n",
        "  for batch_seqs in batch(topredict, 1):\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_seqs)\n",
        "    # build embeddings\n",
        "    # batch_tokens_cuda = batch_tokens.to(device=\"cuda\", non_blocking=True)\n",
        "    batch_tokens_cuda = batch_tokens.to(device, non_blocking=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      results = model(batch_tokens_cuda, repr_layers=[34])\n",
        "    token_embeddings = results[\"representations\"][34]\n",
        "\n",
        "    # Generate per-sequence embeddings via averaging\n",
        "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "    sequence_embeddings = []\n",
        "    for i, (_, seq) in enumerate(batch_seqs):\n",
        "      sequence_embeddings.append(token_embeddings[i, 1:len(seq) + 1].mean(0))\n",
        "\n",
        "    predict_seqs_embeddings=[t.cpu().data.numpy() for t in sequence_embeddings]\n",
        "    preds=[]\n",
        "    for grid in grid_list:\n",
        "      pred = grid.predict(predict_seqs_embeddings)\n",
        "      preds.append(pred)\n",
        "    for i in range(0, len(batch_seqs)):\n",
        "      f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(batch_seqs[i][0], preds[0][i],  preds[1][i],  preds[2][i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w976y3wIAtuB"
      },
      "source": [
        "# PART 3: Emptying cuda cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1FCZMyMkWnd"
      },
      "outputs": [],
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
